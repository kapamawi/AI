{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kapamawi/AI/blob/main/2_1_6_ollama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Przygotowanie"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-19T11:00:41.331748Z",
          "iopub.execute_input": "2024-11-19T11:00:41.332317Z",
          "iopub.status.idle": "2024-11-19T11:00:41.357241Z",
          "shell.execute_reply.started": "2024-11-19T11:00:41.332254Z",
          "shell.execute_reply": "2024-11-19T11:00:41.356264Z"
        },
        "id": "XIFB6H7HUeUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. instalacja Ollamy https://ollama.com/\n",
        "2. z konsoli:  ollama pull llama3.1:latest"
      ],
      "metadata": {
        "id": "Ta1Gk2IQUeUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Skrypt\n",
        "\n",
        "Kod poniżej --> zapisać do pliku app.py"
      ],
      "metadata": {
        "id": "n-owGwUaUeUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "import requests\n",
        "\n",
        "class CFG:\n",
        "    model = 'llama3.1:latest'\n",
        "\n",
        "def get_current_weather(city):\n",
        "    base_url = f\"https://wttr.in/{city}?format=j1\"\n",
        "    response = requests.get(base_url)\n",
        "    data = response.json()\n",
        "    return f\"Temp in {city}: {data['current_condition'][0]['temp_C']}\"\n",
        "\n",
        "\n",
        "def what_is_bigger(n, m):\n",
        "    if n > m:\n",
        "        return f\"{n} is bigger\"\n",
        "    elif m > n:\n",
        "        return f\"{m} is bigger\"\n",
        "    else:\n",
        "        return f\"{n} and {m} are equal\"\n",
        "\n",
        "\n",
        "def chat_with_ollama_no_functions(user_question):\n",
        "    response = ollama.chat(\n",
        "        model = CFG.model,\n",
        "        messages=[\n",
        "            {'role': 'user', 'content': user_question}\n",
        "        ]\n",
        "    )\n",
        "    return response\n",
        "\n",
        "\n",
        "def chat_with_ollama(user_question):\n",
        "    response = ollama.chat(\n",
        "        model = CFG.model,\n",
        "        messages=[\n",
        "            {'role': 'user', 'content': user_question}\n",
        "        ],\n",
        "        tools=[\n",
        "            {\n",
        "                'type': 'function',\n",
        "                'function': {\n",
        "                    'name': \"get_current_weather\",\n",
        "                    'description': \"Get the current weather for a city\",\n",
        "                    'parameters': {\n",
        "                        'type': \"object\",\n",
        "                        'properties': {\n",
        "                            'city': {\n",
        "                                'type': \"string\",\n",
        "                                \"description\": \"City\",\n",
        "                            },\n",
        "                        },\n",
        "                        'required': ['city'],\n",
        "                    },\n",
        "                },\n",
        "            },\n",
        "            {\n",
        "                'type': \"function\",\n",
        "                'function': {\n",
        "                    \"name\": \"which_is_bigger\",\n",
        "                    'parameters': {\n",
        "                        'type': 'object',\n",
        "                        'properties': {\n",
        "                            'n': {\n",
        "                                'type': \"float\",\n",
        "                            },\n",
        "                            \"m\": {\n",
        "                                'type': \"float\"\n",
        "                            },\n",
        "                        },\n",
        "                        'required': ['n', 'm'],\n",
        "                    },\n",
        "                },\n",
        "            },\n",
        "        ],\n",
        "    )\n",
        "    return response\n",
        "\n",
        "\n",
        "def main():\n",
        "    while True:\n",
        "        user_input = input(\"Enter your question (or 'quit' to exit): \")\n",
        "        if user_input.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        response = chat_with_ollama(user_input)\n",
        "\n",
        "        if 'tool_calls' in response['message'] and response['message']['tool_calls']:\n",
        "            tools_calls = response['message']['tool_calls']\n",
        "            for tool_call in tools_calls:\n",
        "                tool_name = tool_call['function']['name']\n",
        "                arguments = tool_call['function']['arguments']\n",
        "\n",
        "                if tool_name == 'get_current_weather' and 'city' in arguments:\n",
        "                    result = get_current_weather(arguments['city'])\n",
        "                    print(\"Weather function result:\", result)\n",
        "                elif tool_name == 'which_is_bigger' and 'n' in arguments and 'm' in arguments:\n",
        "                    n, m = float(arguments['n']), float(arguments['m'])\n",
        "                    result = what_is_bigger(n, m)\n",
        "                    print(\"Comparison function result:\", result)\n",
        "                else:\n",
        "                    print(f\"No valid arguments found for function: {tool_name}\")\n",
        "        else:\n",
        "            # If no tool calls or no valid arguments, use the LLM's response\n",
        "            response = chat_with_ollama_no_functions(user_input)\n",
        "            print(\"AI response:\", response['message']['content'])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-18T00:05:38.487424Z",
          "iopub.execute_input": "2024-11-18T00:05:38.487864Z",
          "iopub.status.idle": "2024-11-18T00:05:46.420106Z",
          "shell.execute_reply.started": "2024-11-18T00:05:38.487805Z",
          "shell.execute_reply": "2024-11-18T00:05:46.418983Z"
        },
        "id": "SpXzVZjYUeUY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kod tworzy interaktywny system czatu wykorzystujący model językowy Ollama z dodatkowymi funkcjami. Przeanalizujmy jego główne elementy:\n",
        "\n",
        "Funkcja `get_current_weather(city)` pobiera aktualną pogodę dla wskazanego miasta:\n",
        "- Łączy się z serwisem wttr.in\n",
        "- Zwraca temperaturę w stopniach Celsjusza dla podanego miasta\n",
        "\n",
        "Funkcja `what_is_bigger(n, m)` porównuje dwie liczby:\n",
        "- Sprawdza która z liczb n i m jest większa\n",
        "- Zwraca odpowiedni komunikat tekstowy z wynikiem porównania\n",
        "\n",
        "Funkcja `chat_with_ollama_no_functions(user_question)` realizuje prostą interakcję z modelem Ollama:\n",
        "- Używa modelu llama3.1:8b-instruct-fp16\n",
        "- Przyjmuje pytanie użytkownika i zwraca odpowiedź modelu\n",
        "\n",
        "Funkcja `chat_with_ollama(user_question)` to rozszerzona wersja czatu:\n",
        "- Wykorzystuje model llama3.1:latest\n",
        "- Ma dostęp do dwóch dodatkowych funkcji (narzędzi):\n",
        "  1. Sprawdzanie pogody\n",
        "  2. Porównywanie liczb\n",
        "- Definiuje strukturę i parametry tych funkcji\n",
        "\n",
        "Funkcja `main()` zarządza interakcją z użytkownikiem:\n",
        "- Działa w pętli do momentu wpisania \"quit\"\n",
        "- Przetwarza wprowadzone pytania\n",
        "- Jeśli model zdecyduje użyć jednej z funkcji:\n",
        "  - Wywołuje odpowiednią funkcję z przekazanymi argumentami\n",
        "  - Wyświetla wynik\n",
        "- W przeciwnym razie:\n",
        "  - Używa podstawowej wersji czatu\n",
        "  - Wyświetla bezpośrednią odpowiedź modelu\n",
        "\n",
        "Kod wykorzystuje biblioteki:\n",
        "- ollama - do komunikacji z modelem językowym\n",
        "- requests - do pobierania danych pogodowych"
      ],
      "metadata": {
        "id": "wrSgNm_tUeUc"
      }
    }
  ]
}