{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kapamawi/AI/blob/main/2_5_2___RAG_router.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "pWVJoc-fRdJB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PW4sRxfMRacL"
      },
      "outputs": [],
      "source": [
        "!pip install -qU llama-index  llama-index-embeddings-huggingface llama-index-llms-groq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ten kod instaluje trzy pakiety Pythona związane z LlamaIndex:\n",
        "\n",
        "- `llama-index` - główna biblioteka do budowania indeksów i przetwarzania danych\n",
        "- `llama-index-embeddings-huggingface` - moduł do generowania embedingów za pomocą modeli HuggingFace\n",
        "- `llama-index-llms-groq` - integracja z modelami językowymi Groq\n",
        "\n",
        "Flaga `-q` wycisza komunikaty instalacji, a `-U` aktualizuje pakiety do najnowszych wersji."
      ],
      "metadata": {
        "id": "TBKVXkLlfhfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.llms.groq import Groq\n",
        "from llama_index.core import Settings, VectorStoreIndex\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "import nest_asyncio\n",
        "from google.colab import userdata\n",
        "from pprint import pp\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "qxA6iQMsRsWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ten kod importuje niezbędne moduły i komponenty:\n",
        "\n",
        "`os` - standardowa biblioteka Pythona do operacji systemowych\n",
        "\n",
        "Z pakietu llama-index.core:\n",
        "- `SimpleDirectoryReader` - czyta dokumenty z katalogu\n",
        "- `SentenceSplitter` - dzieli tekst na zdania\n",
        "- `Settings` - ustawienia globalne\n",
        "- `VectorStoreIndex` - indeks wektorowy do przechowywania dokumentów\n",
        "- `SummaryIndex` - indeks do tworzenia streszczeń\n",
        "- `QueryEngineTool` - narzędzie do przetwarzania zapytań\n",
        "- `RouterQueryEngine` - silnik kierujący zapytania\n",
        "- `LLMSingleSelector` - selektor modelu językowego\n",
        "\n",
        "Dodatkowo:\n",
        "- `Groq` - integracja z modelami Groq\n",
        "- `HuggingFaceEmbedding` - embedingi z HuggingFace\n",
        "- `nest_asyncio` - obsługa zagnieżdżonych pętli asynchronicznych\n",
        "- `userdata` - dostęp do danych użytkownika w Colab\n",
        "- `pp` - funkcja do ładnego wyświetlania struktur danych\n",
        "\n",
        "`nest_asyncio.apply()` włącza obsługę zagnieżdżonych pętli asynchronicznych."
      ],
      "metadata": {
        "id": "uHJbjtTIgfDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "  model1 = \"llama3-8b-8192\"\n",
        "  model2 = \"BAAI/bge-small-en-v1.5\"\n",
        "  temperature = 0.1\n",
        "  chunksize = 1024"
      ],
      "metadata": {
        "id": "nA1w29JMSMcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GROQ_API_KEY'] = userdata.get('groq')"
      ],
      "metadata": {
        "id": "-6E__nm6UV-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data and model"
      ],
      "metadata": {
        "id": "d3nKiMaqUh0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://openreview.net/pdf?id=VtmBAGCN7o\" -O metagpt.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gf8Gy4qRUifn",
        "outputId": "a507ee82-3dde-48b1-b0e4-d852b1da2aea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-28 11:31:35--  https://openreview.net/pdf?id=VtmBAGCN7o\n",
            "Resolving openreview.net (openreview.net)... 35.184.86.251\n",
            "Connecting to openreview.net (openreview.net)|35.184.86.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16911937 (16M) [application/pdf]\n",
            "Saving to: ‘metagpt.pdf’\n",
            "\n",
            "metagpt.pdf         100%[===================>]  16.13M  8.78MB/s    in 1.8s    \n",
            "\n",
            "2024-11-28 11:31:38 (8.78 MB/s) - ‘metagpt.pdf’ saved [16911937/16911937]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To polecenie pobiera dokument PDF z OpenReview.\n",
        "\n",
        "`wget` - narzędzie do pobierania plików z internetu\n",
        "`\"https://openreview.net/pdf?id=VtmBAGCN7o\"` - adres URL pliku PDF\n",
        "`-O metagpt.pdf` - zapisuje pobrany plik pod nazwą \"metagpt.pdf\""
      ],
      "metadata": {
        "id": "N3YqccXQgk7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "documents = SimpleDirectoryReader(input_files=[\"metagpt.pdf\"]).load_data()"
      ],
      "metadata": {
        "id": "ld6gdIiKVwpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ta linia kodu wczytuje zawartość pliku PDF:\n",
        "\n",
        "`SimpleDirectoryReader` to klasa z LlamaIndex służąca do odczytu dokumentów. Dostaje parametr `input_files` z listą zawierającą nazwę pliku \"metagpt.pdf\". Metoda `load_data()` przetwarza plik i ładuje jego zawartość do zmiennej `documents`.\n",
        "\n",
        "Ten kod wykonuje automatyczną ekstrakcję tekstu z PDF-a, co pozwala na dalsze przetwarzanie jego zawartości w formie tekstowej."
      ],
      "metadata": {
        "id": "H9bWSY4tgrqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "llm = Groq(model = CFG.model1)\n",
        "\n",
        "\n",
        "Settings.llm = llm"
      ],
      "metadata": {
        "id": "2k4YJ1qBWGly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ten kod konfiguruje model językowy:\n",
        "\n",
        "`llm = Groq(model = CFG.model1)` - tworzy instancję modelu Groq, używając modelu zdefiniowanego w zmiennej konfiguracyjnej `CFG.model1`\n",
        "\n",
        "`Settings.llm = llm` - ustawia utworzony model jako domyślny model językowy w globalnych ustawieniach LlamaIndex. Dzięki temu wszystkie komponenty będą korzystać z tego samego modelu."
      ],
      "metadata": {
        "id": "-M6PFQ8mhAxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name = CFG.model2\n",
        ")"
      ],
      "metadata": {
        "id": "yfBvanlwV3YL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ta linia kodu konfiguruje model do tworzenia embedingów (reprezentacji wektorowych tekstu):\n",
        "\n",
        "`Settings.embed_model` - ustawia globalny model do embedingów w LlamaIndex\n",
        "\n",
        "Używa się tu `HuggingFaceEmbedding` - implementacji embedingów z biblioteki HuggingFace. Konstruktor przyjmuje parametr `model_name` wskazujący na konkretny model zdefiniowany w zmiennej konfiguracyjnej `CFG.model2`.\n",
        "\n",
        "Takie embedingi będą używane do przekształcania tekstu w wektory liczbowe, co pozwala na wyszukiwanie podobieństw między fragmentami tekstu i efektywne przetwarzanie dokumentów."
      ],
      "metadata": {
        "id": "eQg65UgjhBj3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Narzędzia"
      ],
      "metadata": {
        "id": "wZOj4pSaWWLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "splitter = SentenceSplitter(chunk_size = CFG.chunksize)\n"
      ],
      "metadata": {
        "id": "yH0FECLgXksO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ta linia tworzy obiekt do dzielenia tekstu na mniejsze fragmenty:\n",
        "\n",
        "`SentenceSplitter` to narzędzie, które dzieli tekst na zdania lub fragmenty. Parametr `chunk_size`, ustawiony na wartość z `CFG.chunksize`, określa maksymalną długość pojedynczego fragmentu tekstu.\n",
        "\n",
        "Taki podział tekstu na mniejsze części jest kluczowy dla efektywnego przetwarzania dokumentów, gdyż modele językowe i wektorowe mają ograniczenia co do długości tekstu, który mogą przetworzyć na raz."
      ],
      "metadata": {
        "id": "cUYVlIEChICp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nodes = splitter.get_nodes_from_documents(documents)"
      ],
      "metadata": {
        "id": "-rWdddQ5WXeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ta linia wykonuje podział dokumentów na mniejsze fragmenty tekstowe:\n",
        "\n",
        "`splitter.get_nodes_from_documents(documents)` bierze wcześniej wczytane dokumenty i dzieli je na \"węzły\" (nodes) według wcześniej zdefiniowanych parametrów podziału.\n",
        "\n",
        "Każdy węzeł reprezentuje fragment tekstu o określonej długości (zgodnej z ustawionym `chunk_size`). Węzły zawierają nie tylko tekst, ale również metadane jak pozycja w oryginalnym dokumencie czy relacje z innymi węzłami.\n",
        "\n",
        "Te podzielone fragmenty są zapisywane do zmiennej `nodes` i będą podstawą do budowania indeksu wektorowego."
      ],
      "metadata": {
        "id": "YJyNT0omhRGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "summary_index = SummaryIndex(nodes)"
      ],
      "metadata": {
        "id": "AWcTxlHcXj0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ta linia tworzy indeks streszczeń z wcześniej przygotowanych węzłów tekstowych:\n",
        "\n",
        "`SummaryIndex` to specjalny typ indeksu w LlamaIndex, który jest zoptymalizowany do generowania streszczeń dokumentów. Przyjmuje jako argument `nodes` - wcześniej utworzone fragmenty tekstu.\n",
        "\n",
        "Ten indeks pozwala na generowanie streszczeń całego dokumentu lub jego wybranych części, zachowując przy tym istotne informacje z oryginalnego tekstu."
      ],
      "metadata": {
        "id": "37ebCdGdhU-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vector_index = VectorStoreIndex(nodes)"
      ],
      "metadata": {
        "id": "RCW5Z68hXi3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ta linia tworzy indeks wektorowy z przygotowanych wcześniej węzłów tekstowych:\n",
        "\n",
        "`VectorStoreIndex` zamienia fragmenty tekstu na wektory liczbowe (używając wcześniej skonfigurowanego modelu embedingów) i przechowuje je w strukturze umożliwiającej szybkie wyszukiwanie podobnych fragmentów.\n",
        "\n",
        "Ten indeks jest kluczowy dla efektywnego wyszukiwania semantycznego - pozwala znajdować fragmenty tekstu podobne znaczeniowo do zadanego zapytania, nawet jeśli nie zawierają dokładnie tych samych słów."
      ],
      "metadata": {
        "id": "5ml_Y3ikhVpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_query_engine = summary_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True,\n",
        ")"
      ],
      "metadata": {
        "id": "DFzG6KWOYLNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ta linia tworzy silnik zapytań do indeksu streszczeń:\n",
        "\n",
        "`summary_index.as_query_engine()` zamienia indeks streszczeń na silnik zapytań, który pozwala na interakcję z zawartością dokumentów. Przyjmuje dwa parametry:\n",
        "\n",
        "- `response_mode=\"tree_summarize\"` - używa hierarchicznego podejścia do tworzenia streszczeń, gdzie tekst jest organizowany w strukturę drzewiastą w celu lepszego zachowania kontekstu\n",
        "- `use_async=True` - włącza tryb asynchroniczny, co pozwala na równoległe przetwarzanie i lepszą wydajność\n",
        "\n",
        "Utworzony silnik zapytań jest zapisywany do zmiennej `summary_query_engine` i będzie służył do generowania streszczeń na podstawie zapytań."
      ],
      "metadata": {
        "id": "uOJChOnvhWDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vector_query_engine = vector_index.as_query_engine()"
      ],
      "metadata": {
        "id": "MHU1MhyLYLx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jQib76ELhWfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=summary_query_engine,\n",
        "    description=(\n",
        "        \"Useful for summarization questions related to MetaGPT\"\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "dnFBA6wHYQh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ta linia tworzy silnik zapytań do indeksu wektorowego:\n",
        "\n",
        "`vector_index.as_query_engine()` przekształca indeks wektorowy w silnik zapytań, który umożliwia wyszukiwanie semantyczne w dokumentach.\n",
        "\n",
        "Ten silnik zapytań będzie używał reprezentacji wektorowych tekstu do znajdowania fragmentów dokumentów najbardziej odpowiadających zadanemu pytaniu, wykorzystując podobieństwo wektorów do określania trafności odpowiedzi."
      ],
      "metadata": {
        "id": "uZcXPTjBhW-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_query_engine,\n",
        "    description=(\n",
        "        \"Useful for retrieving specific context from the MetaGPT paper.\"\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "lP7WYmQOYTHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ta linia tworzy narzędzie do obsługi zapytań bazujące na silniku wektorowym:\n",
        "\n",
        "`QueryEngineTool.from_defaults()` tworzy narzędzie z dwoma parametrami:\n",
        "\n",
        "- `query_engine=vector_query_engine` - wskazuje, który silnik zapytań ma być używany (w tym przypadku silnik wektorowy)\n",
        "- `description` - zawiera opis przeznaczenia narzędzia: \"Useful for retrieving specific context from the MetaGPT paper\" (Przydatne do wyszukiwania konkretnych informacji z artykułu o MetaGPT)\n",
        "\n",
        "To narzędzie będzie używane do wyszukiwania konkretnych fragmentów tekstu z dokumentu na podstawie zapytań użytkownika."
      ],
      "metadata": {
        "id": "0mnL3pSChXi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "query_engine = RouterQueryEngine(\n",
        "    selector=LLMSingleSelector.from_defaults(),\n",
        "    query_engine_tools=[\n",
        "        summary_tool,\n",
        "        vector_tool,\n",
        "    ],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# można zawinąć!"
      ],
      "metadata": {
        "id": "Kn2aOBCeZEVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ten kod tworzy zaawansowany silnik zapytań, który potrafi kierować pytania do odpowiednich narzędzi:\n",
        "\n",
        "`RouterQueryEngine` to router zapytań, który otrzymuje trzy parametry:\n",
        "\n",
        "- `selector=LLMSingleSelector.from_defaults()` - selektor wykorzystujący model językowy do wyboru najlepszego narzędzia dla danego zapytania\n",
        "\n",
        "- `query_engine_tools` - lista dostępnych narzędzi do przetwarzania zapytań:\n",
        "  - `summary_tool` - do generowania streszczeń\n",
        "  - `vector_tool` - do wyszukiwania konkretnych informacji\n",
        "\n",
        "- `verbose=True` - włącza szczegółowe logowanie, co pozwala śledzić proces wyboru i przetwarzania zapytań\n",
        "\n",
        "Ten router automatycznie wybiera najlepsze narzędzie do obsługi konkretnego zapytania - albo generowanie streszczenia, albo wyszukiwanie szczegółowych informacji.\n",
        "\n",
        "Komentarz \"można zawinąć!\" oznacza, że kod jest gotowy do użycia i można rozpocząć zadawanie pytań do dokumentu."
      ],
      "metadata": {
        "id": "sK5LD_fphYId"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "Uch6LvV6ZIYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"what is this document about?\")\n",
        "pp(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6LVt2JqZNwg",
        "outputId": "1e985d5c-f0af-495e-9955-4d6376a9fc44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 0: The question is asking about the purpose of the document, and the options suggest that it is related to summarization, which is more relevant to the first option..\n",
            "\u001b[0m('A meta-programming framework called MetaGPT, designed for multi-agent '\n",
            " 'collaboration based on Large Language Models (LLMs), which aims to improve '\n",
            " 'the efficiency and effectiveness of LLM-based multi-agent systems by '\n",
            " 'incorporating Standard Operating Procedures (SOPs) and structured '\n",
            " 'communication interfaces.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"How do agents share information with other agents?\"\n",
        ")\n",
        "pp(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QQPkZGXacw7",
        "outputId": "8ae23471-04af-4300-fd16-409a5210f179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 0: The question is about summarization, which is related to MetaGPT, making choice 1 the most relevant..\n",
            "\u001b[0m('Through a shared message pool, where they can publish structured messages '\n",
            " 'and access messages from other entities transparently.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"How do agents share information with other agents? Do not summarize\"\n",
        ")\n",
        "pp(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjFZ8bucbt2j",
        "outputId": "29a47093-97e9-4c23-dc62-e4c7cb2e45f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 1: The question is asking about retrieving specific context from the MetaGPT paper, which suggests that the question is focused on the technical details of the paper rather than summarization..\n",
            "\u001b[0m('Agents share information with other agents through a shared message pool, '\n",
            " 'where they publish structured messages and can also subscribe to relevant '\n",
            " 'messages based on their profiles.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"How do agents share information with other agents? Explain in detail\"\n",
        ")\n",
        "pp(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Cmx1uiQafKf",
        "outputId": "a189d92c-d43d-448a-b8bf-d7d85dd82355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 0: The question is asking for a detailed explanation, which is more suitable for summarization questions, making choice 1 the most relevant..\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._achat in 0.8575599214643332 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-8b-8192` in organization `org_01jds3pefjfhjb5dj6g9yyqajk` on tokens per minute (TPM): Limit 30000, Used 29820, Requested 3703. Please try again in 7.046s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n",
            "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._achat in 0.06565931800050018 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-8b-8192` in organization `org_01jds3pefjfhjb5dj6g9yyqajk` on tokens per minute (TPM): Limit 30000, Used 29759, Requested 3752. Please try again in 7.021s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n",
            "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._achat in 0.7915937887648932 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-8b-8192` in organization `org_01jds3pefjfhjb5dj6g9yyqajk` on tokens per minute (TPM): Limit 30000, Used 29287, Requested 4037. Please try again in 6.648s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n",
            "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._achat in 0.8413372820245867 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-8b-8192` in organization `org_01jds3pefjfhjb5dj6g9yyqajk` on tokens per minute (TPM): Limit 30000, Used 29118, Requested 3949. Please try again in 6.134s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n"
          ]
        }
      ]
    }
  ]
}